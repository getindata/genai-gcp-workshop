{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering with Documents using LangChain ü¶úÔ∏èüîó and Vertex AI Matching Engine\n",
        "\n",
        "## Generative AI workshop\n",
        "\n",
        "## üîóüîóüîó Connect to the prepared Vector Search / Matching Engine index üîóüîóüîó\n",
        "\n",
        "Based on: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documents_langchain_matching_engine.ipynb\n"
      ],
      "metadata": {
        "id": "htN3_iucNEaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ],
      "metadata": {
        "id": "8vVmp2-iZiTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Vertex AI SDK, other packages and their dependencies\n",
        "Install the following packages required to execute this notebook."
      ],
      "metadata": {
        "id": "_MVZYKKSNNCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QH8HMuIiM6H3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca9ae42-3fa7-44c2-c7ac-ebc27ce8b87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform==1.31.0 in /root/.local/lib/python3.10/site-packages (1.31.0)\n",
            "Requirement already satisfied: langchain==0.0.201 in /root/.local/lib/python3.10/site-packages (0.0.201)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.31.0) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.31.0) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.31.0) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.31.0) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.31.0) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.31.0) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /root/.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.31.0) (1.10.4)\n",
            "Requirement already satisfied: shapely<2.0.0 in /root/.local/lib/python3.10/site-packages (from google-cloud-aiplatform==1.31.0) (1.8.5.post1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /root/.local/lib/python3.10/site-packages (from langchain==0.0.201) (0.5.14)\n",
            "Requirement already satisfied: langchainplus-sdk>=0.0.9 in /root/.local/lib/python3.10/site-packages (from langchain==0.0.201) (0.0.20)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (1.23.5)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /root/.local/lib/python3.10/site-packages (from langchain==0.0.201) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.201) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.201) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.201) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.201) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.201) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.201) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.201) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.201) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.201) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (2.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (1.57.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.31.0) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.31.0) (2.6.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.31.0) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.31.0) (0.12.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.201) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.201) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.201) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.201) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.201) (2.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.31.0) (1.5.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.201) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.31.0) (0.5.0)\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Requirement already satisfied: unstructured==0.7.5 in /root/.local/lib/python3.10/site-packages (0.7.5)\n",
            "Requirement already satisfied: pdf2image==1.16.3 in /root/.local/lib/python3.10/site-packages (1.16.3)\n",
            "Requirement already satisfied: pytesseract==0.3.10 in /root/.local/lib/python3.10/site-packages (0.3.10)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /root/.local/lib/python3.10/site-packages (20221105)\n",
            "Requirement already satisfied: argilla in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (1.16.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (5.2.0)\n",
            "Requirement already satisfied: filetype in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (4.9.3)\n",
            "Requirement already satisfied: msg-parser in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (3.8.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (9.4.0)\n",
            "Requirement already satisfied: pypandoc in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (1.11)\n",
            "Requirement already satisfied: python-docx in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (0.8.11)\n",
            "Requirement already satisfied: python-pptx in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (0.6.22)\n",
            "Requirement already satisfied: python-magic in /root/.local/lib/python3.10/site-packages (from unstructured==0.7.5) (0.4.27)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (3.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (2.31.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (0.9.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured==0.7.5) (2.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract==0.3.10) (23.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.15.1)\n",
            "Requirement already satisfied: httpx<0.24,>=0.15 in /root/.local/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (0.23.3)\n",
            "Requirement already satisfied: deprecated~=1.2.0 in /root/.local/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (1.2.14)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.10.7 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.7.5) (1.10.12)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.13 in /root/.local/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (1.14.1)\n",
            "Requirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.7.5) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.7.5) (4.66.1)\n",
            "Requirement already satisfied: backoff in /root/.local/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (2.2.1)\n",
            "Requirement already satisfied: monotonic in /root/.local/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (1.6)\n",
            "Requirement already satisfied: rich!=13.1.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.7.5) (13.5.2)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.6.0 in /root/.local/lib/python3.10/site-packages (from argilla->unstructured==0.7.5) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured==0.7.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured==0.7.5) (2023.3.post1)\n",
            "Requirement already satisfied: olefile>=0.46 in /root/.local/lib/python3.10/site-packages (from msg-parser->unstructured==0.7.5) (0.46)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.7.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.7.5) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.7.5) (2023.6.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured==0.7.5) (1.1.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /root/.local/lib/python3.10/site-packages (from python-pptx->unstructured==0.7.5) (3.1.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.7.5) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.7.5) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured==0.7.5) (2023.7.22)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.21)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /root/.local/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured==0.7.5) (0.16.3)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /root/.local/lib/python3.10/site-packages (from httpx<0.24,>=0.15->argilla->unstructured==0.7.5) (1.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.24,>=0.15->argilla->unstructured==0.7.5) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0,>=1.10.7->argilla->unstructured==0.7.5) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured==0.7.5) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich!=13.1.0->argilla->unstructured==0.7.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich!=13.1.0->argilla->unstructured==0.7.5) (2.16.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /root/.local/lib/python3.10/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured==0.7.5) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured==0.7.5) (3.7.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich!=13.1.0->argilla->unstructured==0.7.5) (0.1.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured==0.7.5) (1.1.3)\n",
            "Collecting tensorflow_hub==0.13.0\n",
            "  Using cached tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
            "Collecting tensorflow_text==2.12.1\n",
            "  Using cached tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub==0.13.0) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub==0.13.0) (3.20.3)\n",
            "Collecting tensorflow<2.13,>=2.12.0 (from tensorflow_text==2.12.1)\n",
            "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.4.14)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /root/.local/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.11.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow_text==2.12.1) (3.2.2)\n",
            "Installing collected packages: tensorflow_hub, tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow_text\n",
            "\u001b[33m  WARNING: The scripts make_image_classifier and make_nearest_neighbour_index are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed keras-2.12.0 tensorboard-2.12.3 tensorflow-2.12.1 tensorflow-estimator-2.12.0 tensorflow_hub-0.13.0 tensorflow_text-2.12.1\n"
          ]
        }
      ],
      "source": [
        "# Install Vertex AI LLM SDK\n",
        "! pip install --user --upgrade google-cloud-aiplatform==1.31.0 langchain==0.0.201\n",
        "\n",
        "# Dependencies required by Unstructured PDF loader\n",
        "! sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "! sudo apt-get -y -qq install poppler-utils\n",
        "! pip install --user unstructured==0.7.5 pdf2image==1.16.3 pytesseract==0.3.10 pdfminer.six==20221105\n",
        "\n",
        "# For Matching Engine integration dependencies (default embeddings)\n",
        "! pip install --user tensorflow_hub==0.13.0 tensorflow_text==2.12.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1VIpX9JNZxa",
        "outputId": "e9920474-102e-4c1c-f013-bb3dce1a97f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "PHMtSQp1NmjR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"utils\"):\n",
        "    os.makedirs(\"utils\")\n",
        "\n",
        "url_prefix = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/use-cases/document-qa/utils\"\n",
        "files = [\"__init__.py\", \"matching_engine.py\", \"matching_engine_utils.py\"]\n",
        "\n",
        "for fname in files:\n",
        "    urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"utils/{fname}\")"
      ],
      "metadata": {
        "id": "RbVh249eNq4Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "# Utils\n",
        "import time\n",
        "import uuid\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import vertexai\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import GCSDirectoryLoader\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Import custom Matching Engine packages\n",
        "from utils.matching_engine import MatchingEngine\n",
        "from utils.matching_engine_utils import MatchingEngineUtils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBgJtdsRNwuN",
        "outputId": "1123df9b-a47c-4353-917f-ee2f4c607c72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.31.0\n",
            "LangChain version: 0.0.201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"datamass-2023-genai\"  # @param {type:\"string\"}\n",
        "REGION = \"europe-west1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "BS4-xNcfNzDe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "7f2ivAc_Zw69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]"
      ],
      "metadata": {
        "id": "NzPFRZVoN25k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize LangChain Models"
      ],
      "metadata": {
        "id": "Q8lKemrWaALX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text model instance integrated with langChain\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Embeddings API integrated with langChain\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        ")"
      ],
      "metadata": {
        "id": "wJj5qNKiN51A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to the pre-built Matching Engine index"
      ],
      "metadata": {
        "id": "-tBiRUAFaMbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ME_REGION = \"europe-west1\"\n",
        "ME_INDEX_NAME = \"datamass-2023-genai-mwiewior-index\"  # @param {type:\"string\"}\n",
        "ME_EMBEDDING_DIR = \"datamass-2023-genai-mwiewior-bucket\"  # @param {type:\"string\"}\n",
        "ME_DIMENSIONS = 768  # when using Vertex PaLM Embedding"
      ],
      "metadata": {
        "id": "tpKyhy6_N-VX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mengine = MatchingEngineUtils(PROJECT_ID, ME_REGION, ME_INDEX_NAME)"
      ],
      "metadata": {
        "id": "5tMWuE5lOT3O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = mengine.get_index_and_endpoint()\n",
        "\n",
        "# temp override\n",
        "ME_INDEX_ENDPOINT_ID = 'projects/716156477321/locations/europe-west1/indexEndpoints/3806790730337222656'\n",
        "\n",
        "print(f\"ME_INDEX_ID={ME_INDEX_ID}\")\n",
        "print(f\"ME_INDEX_ENDPOINT_ID={ME_INDEX_ENDPOINT_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrcOJs9_QhXw",
        "outputId": "827ed56c-4473-4177-a9c4-701098d3619d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ME_INDEX_ID=projects/716156477321/locations/europe-west1/indexes/7681680812852379648\n",
            "ME_INDEX_ENDPOINT_ID=projects/716156477321/locations/europe-west1/indexEndpoints/3806790730337222656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "me = MatchingEngine.from_components(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=ME_REGION,\n",
        "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_DIR}\".split(\"/\")[2],\n",
        "    embedding=embeddings,\n",
        "    index_id=ME_INDEX_ID,\n",
        "    endpoint_id=ME_INDEX_ENDPOINT_ID\n",
        ")"
      ],
      "metadata": {
        "id": "VNSxOB1gQij6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate semantic search with Matching Engine is working"
      ],
      "metadata": {
        "id": "H5nUObCrahJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "me.similarity_search(\"how to integrate kedro with AzureML?\", k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCAPzjrAQymH",
        "outputId": "63e899f1-2198-4d66-e2f3-be52a9d1136c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Running Kedro on Azure Machine Learning Pipelines\\n\\nGitHub: https://github.com/getindata/kedro-azureml Documentation: https://kedro-azureml.readthedocs.io/en/stable/\\n\\nIf your cloud platform of choice is Azure, we've also got your back! Just recently (August 2022) we published the Kedro-AzureML plugin that enables you to run Kedro pipelines in the Azure Machine Learning Pipelines service. Similar to Vertex AI - Azure ML Pipelines is also a fully managed service that provides easy scale-up capabilities. ML Engineers need to set-up compute clusters first, but it‚Äôs an easy process, which can be handled without the involvement of the DevOps teams. Once done, Kedro pipelines can be executed on Azure ML Pipelines. Moreover, metrics and trained models can be easily tracked using Azure ML‚Äôs built-in MLflow integration, without any additional configuration and our plug-in supports that.\", metadata={'source': 'datamass-lab-2023/', 'document_name': 'Running Kedro... everywhere - blogpost.pdf', 'chunk': '56', 'score': 0.8036769032478333}),\n",
              " Document(page_content=\"Running Kedro on Azure Machine Learning Pipelines\\n\\nGitHub: https://github.com/getindata/kedro-azureml Documentation: https://kedro-azureml.readthedocs.io/en/stable/\\n\\nIf your cloud platform of choice is Azure, we've also got your back! Just recently (August 2022) we published the Kedro-AzureML plugin that enables you to run Kedro pipelines in the Azure Machine Learning Pipelines service. Similar to Vertex AI - Azure ML Pipelines is also a fully managed service that provides easy scale-up capabilities. ML Engineers need to set-up compute clusters first, but it‚Äôs an easy process, which can be handled without the involvement of the DevOps teams. Once done, Kedro pipelines can be executed on Azure ML Pipelines. Moreover, metrics and trained models can be easily tracked using Azure ML‚Äôs built-in MLflow integration, without any additional configuration and our plug-in supports that.\", metadata={'source': 'datamass-lab-2023/', 'document_name': 'Running Kedro... everywhere - blogpost.pdf', 'chunk': '56', 'score': 0.8036769032478333})]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "me.similarity_search(\"How to integrate MLflow with Snowflake?\", k=2, search_distance=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-6knHllQ2Ph",
        "outputId": "b03fc0e0-cc68-4110-d1d9-eda743364ee1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='sent/received from the MLflow instance.\\n\\nSnowflake API integration for setting up a communication channel from the\\n\\nSnowflake instance to the cloud HTTPS proxy/gateway service where your\\n\\nMLflow instance is hosted (e.g. Amazon API Gateway, Google Cloud API\\n\\nGateway or Azure API Management).\\n\\nSnowflake storage integration to enable your Snowflake instance to upload\\n\\nartifacts (e.g. serialized models) to the cloud storage (Amazon S3, Azure Blob\\n\\nStorage, Google Cloud Storage) used by the MLflow instance.\\n\\nOffline model deployment within Snowflake is handled by the MLflow-Snowflake\\n\\nplugin, which allows you to deploy models trained in popular frameworks (such as\\n\\nPyTorch, Scikit-Learn, TensorFlow, LightGBM, XGBoost, ONNX and others)\\n\\nnatively in Snowflake as User-Defined Functions, which then can be called\\n\\ndirectly from SQL to efficiently (using vectorization) perform inference and obtain\\n\\nthe models‚Äô predictions. The plugin is backed by the Snowflake team and is\\n\\nactively developed.', metadata={'source': 'datamass-lab-2023/', 'document_name': 'MLOps@Snowflake blog post.pdf', 'chunk': '43', 'score': 0.8213270306587219}),\n",
              " Document(page_content='sent/received from the MLflow instance.\\n\\nSnowflake API integration for setting up a communication channel from the\\n\\nSnowflake instance to the cloud HTTPS proxy/gateway service where your\\n\\nMLflow instance is hosted (e.g. Amazon API Gateway, Google Cloud API\\n\\nGateway or Azure API Management).\\n\\nSnowflake storage integration to enable your Snowflake instance to upload\\n\\nartifacts (e.g. serialized models) to the cloud storage (Amazon S3, Azure Blob\\n\\nStorage, Google Cloud Storage) used by the MLflow instance.\\n\\nOffline model deployment within Snowflake is handled by the MLflow-Snowflake\\n\\nplugin, which allows you to deploy models trained in popular frameworks (such as\\n\\nPyTorch, Scikit-Learn, TensorFlow, LightGBM, XGBoost, ONNX and others)\\n\\nnatively in Snowflake as User-Defined Functions, which then can be called\\n\\ndirectly from SQL to efficiently (using vectorization) perform inference and obtain\\n\\nthe models‚Äô predictions. The plugin is backed by the Snowflake team and is\\n\\nactively developed.', metadata={'source': 'datamass-lab-2023/', 'document_name': 'MLOps@Snowflake blog post.pdf', 'chunk': '43', 'score': 0.8213270306587219})]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval based Question/Answering Chain"
      ],
      "metadata": {
        "id": "jEcDhGG3apC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Question/Answering Chain with Vector Store using Text"
      ],
      "metadata": {
        "id": "HWLAyPAmaupI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chain to answer questions\n",
        "NUMBER_OF_RESULTS = 10\n",
        "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
        "\n",
        "# Expose index to the retriever\n",
        "retriever = me.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": NUMBER_OF_RESULTS,\n",
        "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "B-w_x1fiQ_K0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"SYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
        "\n",
        "Do not try to make up an answer:\n",
        " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
        " - If the context is empty, just say \"I do not know the answer to that.\"\n",
        "\n",
        "=============\n",
        "{context}\n",
        "=============\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\""
      ],
      "metadata": {
        "id": "29reaQFNRJYv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses LLM to synthesize results from the search index.\n",
        "# Use Vertex PaLM Text API for LLM\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "        ),\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "Kscxwff9ROmo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable for troubleshooting\n",
        "qa.combine_documents_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.verbose = True\n",
        "qa.combine_documents_chain.llm_chain.llm.verbose = True"
      ],
      "metadata": {
        "id": "YJvB4ZhSRQ3V"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatter(result):\n",
        "    print(f\"Query: {result['query']}\")\n",
        "    print(\".\" * 80)\n",
        "    if \"source_documents\" in result.keys():\n",
        "        for idx, ref in enumerate(result[\"source_documents\"]):\n",
        "            print(\"-\" * 80)\n",
        "            print(f\"REFERENCE #{idx}\")\n",
        "            print(\"-\" * 80)\n",
        "            if \"score\" in ref.metadata:\n",
        "                print(f\"Matching Score: {ref.metadata['score']}\")\n",
        "            if \"source\" in ref.metadata:\n",
        "                print(f\"Document Source: {ref.metadata['source']}\")\n",
        "            if \"document_name\" in ref.metadata:\n",
        "                print(f\"Document Name: {ref.metadata['document_name']}\")\n",
        "            print(\".\" * 80)\n",
        "            print(f\"Content: \\n{wrap(ref.page_content)}\")\n",
        "    print(\".\" * 80)\n",
        "    print(f\"Response: {wrap(result['result'])}\")\n",
        "    print(\".\" * 80)\n",
        "\n",
        "\n",
        "def wrap(s):\n",
        "    return \"\\n\".join(textwrap.wrap(s, width=120, break_long_words=False))\n",
        "\n",
        "\n",
        "def ask(query, qa=qa, k=NUMBER_OF_RESULTS, search_distance=SEARCH_DISTANCE_THRESHOLD):\n",
        "    qa.retriever.search_kwargs[\"search_distance\"] = search_distance\n",
        "    qa.retriever.search_kwargs[\"k\"] = k\n",
        "    result = qa({\"query\": query})\n",
        "    return formatter(result)"
      ],
      "metadata": {
        "id": "4kCt7a3KRTHt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run QA chain on sample questions"
      ],
      "metadata": {
        "id": "WwmimrSia2jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NHfWpoifa62m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"How to integrate kedro with AzureML?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFOqSUvFRTlU",
        "outputId": "78f3d49d-3831-49cd-84da-dae6b1dcff4a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Waiting\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
            "\n",
            "Question: How to integrate kedro with AzureML?\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "Running Kedro on Azure Machine Learning Pipelines\n",
            "\n",
            "GitHub: https://github.com/getindata/kedro-azureml Documentation: https://kedro-azureml.readthedocs.io/en/stable/\n",
            "\n",
            "If your cloud platform of choice is Azure, we've also got your back! Just recently (August 2022) we published the Kedro-AzureML plugin that enables you to run Kedro pipelines in the Azure Machine Learning Pipelines service. Similar to Vertex AI - Azure ML Pipelines is also a fully managed service that provides easy scale-up capabilities. ML Engineers need to set-up compute clusters first, but it‚Äôs an easy process, which can be handled without the involvement of the DevOps teams. Once done, Kedro pipelines can be executed on Azure ML Pipelines. Moreover, metrics and trained models can be easily tracked using Azure ML‚Äôs built-in MLflow integration, without any additional configuration and our plug-in supports that.\n",
            "\n",
            "Running Kedro on Azure Machine Learning Pipelines\n",
            "\n",
            "GitHub: https://github.com/getindata/kedro-azureml Documentation: https://kedro-azureml.readthedocs.io/en/stable/\n",
            "\n",
            "If your cloud platform of choice is Azure, we've also got your back! Just recently (August 2022) we published the Kedro-AzureML plugin that enables you to run Kedro pipelines in the Azure Machine Learning Pipelines service. Similar to Vertex AI - Azure ML Pipelines is also a fully managed service that provides easy scale-up capabilities. ML Engineers need to set-up compute clusters first, but it‚Äôs an easy process, which can be handled without the involvement of the DevOps teams. Once done, Kedro pipelines can be executed on Azure ML Pipelines. Moreover, metrics and trained models can be easily tracked using Azure ML‚Äôs built-in MLflow integration, without any additional configuration and our plug-in supports that.\n",
            "\n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com\n",
            "\n",
            "GCP and Azure deployment scenarios are very much the same except for:\n",
            "\n",
            "API Gateway (GCP) or API Management (Azure) for exposing MLflow API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure\n",
            "\n",
            "Container Instances (Azure)\n",
            "\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints\n",
            "\n",
            "Technical deep-dive into kedro-snowflake <-> MLflow integration\n",
            "\n",
            "Kedro-snowflake <-> MLflow integration is based on the following concepts:\n",
            "\n",
            "Snowflake external functions that are used for wrapping POST requests to the\n",
            "\n",
            "MLflow instance. In the minimal setup the following wrapping external functions\n",
            "\n",
            "for MLflow REST API calls must be created:\n",
            "\n",
            "Create run\n",
            "\n",
            "Update run\n",
            "\n",
            "Log param\n",
            "\n",
            "Log metric\n",
            "\n",
            "Search experiment\n",
            "\n",
            "Snowflake external function translators for changing the format of the data\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com\n",
            "\n",
            "GCP and Azure deployment scenarios are very much the same except for:\n",
            "\n",
            "API Gateway (GCP) or API Management (Azure) for exposing MLflow API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure\n",
            "\n",
            "Container Instances (Azure)\n",
            "\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints\n",
            "\n",
            "Technical deep-dive into kedro-snowflake <-> MLflow integration\n",
            "\n",
            "Kedro-snowflake <-> MLflow integration is based on the following concepts:\n",
            "\n",
            "Snowflake external functions that are used for wrapping POST requests to the\n",
            "\n",
            "MLflow instance. In the minimal setup the following wrapping external functions\n",
            "\n",
            "for MLflow REST API calls must be created:\n",
            "\n",
            "Create run\n",
            "\n",
            "Update run\n",
            "\n",
            "Log param\n",
            "\n",
            "Log metric\n",
            "\n",
            "Search experiment\n",
            "\n",
            "Snowflake external function translators for changing the format of the data\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our GitHub repos).\n",
            "\n",
            "Z komentarzem [3]: dajmy bitly na to https://github.com/getindata\n",
            "\n",
            "The below table summarizes the pros and cons of different approaches:\n",
            "\n",
            "SageMaker/Azure ML/Vertex AI\n",
            "\n",
            "Airflow\n",
            "\n",
            "Snowflake/Snowpark\n",
            "\n",
            "Orchestration\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Native data processing\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "Docker support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Native ML capabilities\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Model deployment support (serving)\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Maintenance\n",
            "\n",
            "High\n",
            "\n",
            "Low (serverless)\n",
            "\n",
            "Low / Medium\n",
            "\n",
            "Extensibility / Customizability\n",
            "\n",
            "High\n",
            "\n",
            "Low\n",
            "\n",
            "Medium\n",
            "\n",
            "Performance\n",
            "\n",
            "Depends on setup\n",
            "\n",
            "Varies\n",
            "\n",
            "Low to very high\n",
            "\n",
            "Experiment tracking\n",
            "\n",
            "External\n",
            "\n",
            "Built-in\n",
            "\n",
            "External\n",
            "\n",
            "GPU support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Python + Any (Docker)\n",
            "\n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our GitHub repos).\n",
            "\n",
            "Z komentarzem [3]: dajmy bitly na to https://github.com/getindata\n",
            "\n",
            "The below table summarizes the pros and cons of different approaches:\n",
            "\n",
            "SageMaker/Azure ML/Vertex AI\n",
            "\n",
            "Airflow\n",
            "\n",
            "Snowflake/Snowpark\n",
            "\n",
            "Orchestration\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Native data processing\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "Docker support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Native ML capabilities\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Model deployment support (serving)\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Maintenance\n",
            "\n",
            "High\n",
            "\n",
            "Low (serverless)\n",
            "\n",
            "Low / Medium\n",
            "\n",
            "Extensibility / Customizability\n",
            "\n",
            "High\n",
            "\n",
            "Low\n",
            "\n",
            "Medium\n",
            "\n",
            "Performance\n",
            "\n",
            "Depends on setup\n",
            "\n",
            "Varies\n",
            "\n",
            "Low to very high\n",
            "\n",
            "Experiment tracking\n",
            "\n",
            "External\n",
            "\n",
            "Built-in\n",
            "\n",
            "External\n",
            "\n",
            "GPU support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Python + Any (Docker)\n",
            "\n",
            "Kedro Spaceflights Tutorial running on Azure Machine Learning Pipelines Kasia - ALT \"machine learning pipelines kedro azure ml plugin getindata\"\n",
            "\n",
            "We encourage you to watch this step-by-step tutorial: https://m.youtube.com/watch?v=w_9RzYpGplY about the Kedro-AzureML plugin that enables you to run Kedro pipelines on Azure ML Pipelines service.\n",
            "\n",
            "Summary\n",
            "\n",
            "We highly encourage you to try out Kedro in your machine learning projects and leverage one of our open-source plug-ins to run and scale Machine Learning pipelines in the cloud! Feel free to reach out to us on GitHub if you encounter any issues with the plug-ins, we‚Äôre always happy to help. Happy coding!\n",
            "\n",
            "Would you like to stay up-to date with our plugin publications? Subscribe to our newsletter and don't miss out on anything!\n",
            "\n",
            "Kedro Spaceflights Tutorial running on Azure Machine Learning Pipelines Kasia - ALT \"machine learning pipelines kedro azure ml plugin getindata\"\n",
            "\n",
            "We encourage you to watch this step-by-step tutorial: https://m.youtube.com/watch?v=w_9RzYpGplY about the Kedro-AzureML plugin that enables you to run Kedro pipelines on Azure ML Pipelines service.\n",
            "\n",
            "Summary\n",
            "\n",
            "We highly encourage you to try out Kedro in your machine learning projects and leverage one of our open-source plug-ins to run and scale Machine Learning pipelines in the cloud! Feel free to reach out to us on GitHub if you encounter any issues with the plug-ins, we‚Äôre always happy to help. Happy coding!\n",
            "\n",
            "Would you like to stay up-to date with our plugin publications? Subscribe to our newsletter and don't miss out on anything!\n",
            "\n",
            "The above list of challenges clearly indicates missing the integration of the Snowflake environment with an MLOps framework, such as Kedro.\n",
            "\n",
            "Today we are proudly announcing a solution that will fill this gap - the kedro-snowflake plugin. In the next post we will also guide you through the whole MLOps platform and ML model deployment on Snowflake. However, let's first take a look at what Kedro is and then let's build an ML pipeline in Kedro and execute it in the Snowflake environment in 3 simple steps.\n",
            "\n",
            "Kedro - the MLOps Framework\n",
            "\n",
            "Kedro is a widely-adopted, open-source Python framework that has claimed to bring engineering back to the data science world. The rationale behind using Kedro as a framework for creating maintainable and modular training code is in many aspects, similar to preferring Terraform technology over cloud-vendor native SDK for infrastructure provisioning and can be summarized in the following points:\n",
            "\n",
            "The above list of challenges clearly indicates missing the integration of the Snowflake environment with an MLOps framework, such as Kedro.\n",
            "\n",
            "Today we are proudly announcing a solution that will fill this gap - the kedro-snowflake plugin. In the next post we will also guide you through the whole MLOps platform and ML model deployment on Snowflake. However, let's first take a look at what Kedro is and then let's build an ML pipeline in Kedro and execute it in the Snowflake environment in 3 simple steps.\n",
            "\n",
            "Kedro - the MLOps Framework\n",
            "\n",
            "Kedro is a widely-adopted, open-source Python framework that has claimed to bring engineering back to the data science world. The rationale behind using Kedro as a framework for creating maintainable and modular training code is in many aspects, similar to preferring Terraform technology over cloud-vendor native SDK for infrastructure provisioning and can be summarized in the following points:\n",
            "=============\n",
            "\n",
            "Question: How to integrate kedro with AzureML?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: How to integrate kedro with AzureML?\n",
            "................................................................................\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #0\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.8045610785484314\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: Running Kedro... everywhere - blogpost.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Running Kedro on Azure Machine Learning Pipelines  GitHub: https://github.com/getindata/kedro-azureml Documentation:\n",
            "https://kedro-azureml.readthedocs.io/en/stable/  If your cloud platform of choice is Azure, we've also got your back!\n",
            "Just recently (August 2022) we published the Kedro-AzureML plugin that enables you to run Kedro pipelines in the Azure\n",
            "Machine Learning Pipelines service. Similar to Vertex AI - Azure ML Pipelines is also a fully managed service that\n",
            "provides easy scale-up capabilities. ML Engineers need to set-up compute clusters first, but it‚Äôs an easy process, which\n",
            "can be handled without the involvement of the DevOps teams. Once done, Kedro pipelines can be executed on Azure ML\n",
            "Pipelines. Moreover, metrics and trained models can be easily tracked using Azure ML‚Äôs built-in MLflow integration,\n",
            "without any additional configuration and our plug-in supports that.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #1\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.8045610785484314\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: Running Kedro... everywhere - blogpost.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Running Kedro on Azure Machine Learning Pipelines  GitHub: https://github.com/getindata/kedro-azureml Documentation:\n",
            "https://kedro-azureml.readthedocs.io/en/stable/  If your cloud platform of choice is Azure, we've also got your back!\n",
            "Just recently (August 2022) we published the Kedro-AzureML plugin that enables you to run Kedro pipelines in the Azure\n",
            "Machine Learning Pipelines service. Similar to Vertex AI - Azure ML Pipelines is also a fully managed service that\n",
            "provides easy scale-up capabilities. ML Engineers need to set-up compute clusters first, but it‚Äôs an easy process, which\n",
            "can be handled without the involvement of the DevOps teams. Once done, Kedro pipelines can be executed on Azure ML\n",
            "Pipelines. Moreover, metrics and trained models can be easily tracked using Azure ML‚Äôs built-in MLflow integration,\n",
            "without any additional configuration and our plug-in supports that.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #2\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7892988324165344\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com  GCP and Azure\n",
            "deployment scenarios are very much the same except for:  API Gateway (GCP) or API Management (Azure) for exposing MLflow\n",
            "API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure  Container Instances (Azure)\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints  Technical deep-dive into kedro-snowflake <-> MLflow\n",
            "integration  Kedro-snowflake <-> MLflow integration is based on the following concepts:  Snowflake external functions\n",
            "that are used for wrapping POST requests to the  MLflow instance. In the minimal setup the following wrapping external\n",
            "functions  for MLflow REST API calls must be created:  Create run  Update run  Log param  Log metric  Search experiment\n",
            "Snowflake external function translators for changing the format of the data  sent/received from the MLflow instance.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #3\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7892988324165344\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com  GCP and Azure\n",
            "deployment scenarios are very much the same except for:  API Gateway (GCP) or API Management (Azure) for exposing MLflow\n",
            "API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure  Container Instances (Azure)\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints  Technical deep-dive into kedro-snowflake <-> MLflow\n",
            "integration  Kedro-snowflake <-> MLflow integration is based on the following concepts:  Snowflake external functions\n",
            "that are used for wrapping POST requests to the  MLflow instance. In the minimal setup the following wrapping external\n",
            "functions  for MLflow REST API calls must be created:  Create run  Update run  Log param  Log metric  Search experiment\n",
            "Snowflake external function translators for changing the format of the data  sent/received from the MLflow instance.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #4\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.768439769744873\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and\n",
            "switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for\n",
            "Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our\n",
            "GitHub repos).  Z komentarzem [3]: dajmy bitly na to https://github.com/getindata  The below table summarizes the pros\n",
            "and cons of different approaches:  SageMaker/Azure ML/Vertex AI  Airflow  Snowflake/Snowpark  Orchestration  1  1  0.5\n",
            "Native data processing  0  0  1  Docker support  1  1  0  Native ML capabilities  0  1  0.5  Model deployment support\n",
            "(serving)  0  1  0.5  Maintenance  High  Low (serverless)  Low / Medium  Extensibility / Customizability  High  Low\n",
            "Medium  Performance  Depends on setup  Varies  Low to very high  Experiment tracking  External  Built-in  External  GPU\n",
            "support  1  1  0  Python + Any (Docker)\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #5\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.768439769744873\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and\n",
            "switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for\n",
            "Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our\n",
            "GitHub repos).  Z komentarzem [3]: dajmy bitly na to https://github.com/getindata  The below table summarizes the pros\n",
            "and cons of different approaches:  SageMaker/Azure ML/Vertex AI  Airflow  Snowflake/Snowpark  Orchestration  1  1  0.5\n",
            "Native data processing  0  0  1  Docker support  1  1  0  Native ML capabilities  0  1  0.5  Model deployment support\n",
            "(serving)  0  1  0.5  Maintenance  High  Low (serverless)  Low / Medium  Extensibility / Customizability  High  Low\n",
            "Medium  Performance  Depends on setup  Varies  Low to very high  Experiment tracking  External  Built-in  External  GPU\n",
            "support  1  1  0  Python + Any (Docker)\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #6\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7670741081237793\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: Running Kedro... everywhere - blogpost.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Kedro Spaceflights Tutorial running on Azure Machine Learning Pipelines Kasia - ALT \"machine learning pipelines kedro\n",
            "azure ml plugin getindata\"  We encourage you to watch this step-by-step tutorial:\n",
            "https://m.youtube.com/watch?v=w_9RzYpGplY about the Kedro-AzureML plugin that enables you to run Kedro pipelines on\n",
            "Azure ML Pipelines service.  Summary  We highly encourage you to try out Kedro in your machine learning projects and\n",
            "leverage one of our open-source plug-ins to run and scale Machine Learning pipelines in the cloud! Feel free to reach\n",
            "out to us on GitHub if you encounter any issues with the plug-ins, we‚Äôre always happy to help. Happy coding!  Would you\n",
            "like to stay up-to date with our plugin publications? Subscribe to our newsletter and don't miss out on anything!\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #7\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7670741081237793\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: Running Kedro... everywhere - blogpost.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Kedro Spaceflights Tutorial running on Azure Machine Learning Pipelines Kasia - ALT \"machine learning pipelines kedro\n",
            "azure ml plugin getindata\"  We encourage you to watch this step-by-step tutorial:\n",
            "https://m.youtube.com/watch?v=w_9RzYpGplY about the Kedro-AzureML plugin that enables you to run Kedro pipelines on\n",
            "Azure ML Pipelines service.  Summary  We highly encourage you to try out Kedro in your machine learning projects and\n",
            "leverage one of our open-source plug-ins to run and scale Machine Learning pipelines in the cloud! Feel free to reach\n",
            "out to us on GitHub if you encounter any issues with the plug-ins, we‚Äôre always happy to help. Happy coding!  Would you\n",
            "like to stay up-to date with our plugin publications? Subscribe to our newsletter and don't miss out on anything!\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #8\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7566432952880859\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: kedro-snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "The above list of challenges clearly indicates missing the integration of the Snowflake environment with an MLOps\n",
            "framework, such as Kedro.  Today we are proudly announcing a solution that will fill this gap - the kedro-snowflake\n",
            "plugin. In the next post we will also guide you through the whole MLOps platform and ML model deployment on Snowflake.\n",
            "However, let's first take a look at what Kedro is and then let's build an ML pipeline in Kedro and execute it in the\n",
            "Snowflake environment in 3 simple steps.  Kedro - the MLOps Framework  Kedro is a widely-adopted, open-source Python\n",
            "framework that has claimed to bring engineering back to the data science world. The rationale behind using Kedro as a\n",
            "framework for creating maintainable and modular training code is in many aspects, similar to preferring Terraform\n",
            "technology over cloud-vendor native SDK for infrastructure provisioning and can be summarized in the following points:\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #9\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7566432952880859\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: kedro-snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "The above list of challenges clearly indicates missing the integration of the Snowflake environment with an MLOps\n",
            "framework, such as Kedro.  Today we are proudly announcing a solution that will fill this gap - the kedro-snowflake\n",
            "plugin. In the next post we will also guide you through the whole MLOps platform and ML model deployment on Snowflake.\n",
            "However, let's first take a look at what Kedro is and then let's build an ML pipeline in Kedro and execute it in the\n",
            "Snowflake environment in 3 simple steps.  Kedro - the MLOps Framework  Kedro is a widely-adopted, open-source Python\n",
            "framework that has claimed to bring engineering back to the data science world. The rationale behind using Kedro as a\n",
            "framework for creating maintainable and modular training code is in many aspects, similar to preferring Terraform\n",
            "technology over cloud-vendor native SDK for infrastructure provisioning and can be summarized in the following points:\n",
            "................................................................................\n",
            "Response: To integrate kedro with AzureML, you can use the Kedro-AzureML plugin. This plugin enables you to run Kedro pipelines on\n",
            "Azure ML Pipelines service. To learn more about the plugin, please refer to the documentation: https://kedro-\n",
            "azureml.readthedocs.io/en/stable/\n",
            "................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"How to integrate kedro with MLFlow?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzcJREAeRVrH",
        "outputId": "e92659fa-6a8e-417e-ffbe-b9dc8dfe815b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Waiting\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
            "\n",
            "Question: How to integrate kedro with MLFlow?\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com\n",
            "\n",
            "GCP and Azure deployment scenarios are very much the same except for:\n",
            "\n",
            "API Gateway (GCP) or API Management (Azure) for exposing MLflow API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure\n",
            "\n",
            "Container Instances (Azure)\n",
            "\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints\n",
            "\n",
            "Technical deep-dive into kedro-snowflake <-> MLflow integration\n",
            "\n",
            "Kedro-snowflake <-> MLflow integration is based on the following concepts:\n",
            "\n",
            "Snowflake external functions that are used for wrapping POST requests to the\n",
            "\n",
            "MLflow instance. In the minimal setup the following wrapping external functions\n",
            "\n",
            "for MLflow REST API calls must be created:\n",
            "\n",
            "Create run\n",
            "\n",
            "Update run\n",
            "\n",
            "Log param\n",
            "\n",
            "Log metric\n",
            "\n",
            "Search experiment\n",
            "\n",
            "Snowflake external function translators for changing the format of the data\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com\n",
            "\n",
            "GCP and Azure deployment scenarios are very much the same except for:\n",
            "\n",
            "API Gateway (GCP) or API Management (Azure) for exposing MLflow API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure\n",
            "\n",
            "Container Instances (Azure)\n",
            "\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints\n",
            "\n",
            "Technical deep-dive into kedro-snowflake <-> MLflow integration\n",
            "\n",
            "Kedro-snowflake <-> MLflow integration is based on the following concepts:\n",
            "\n",
            "Snowflake external functions that are used for wrapping POST requests to the\n",
            "\n",
            "MLflow instance. In the minimal setup the following wrapping external functions\n",
            "\n",
            "for MLflow REST API calls must be created:\n",
            "\n",
            "Create run\n",
            "\n",
            "Update run\n",
            "\n",
            "Log param\n",
            "\n",
            "Log metric\n",
            "\n",
            "Search experiment\n",
            "\n",
            "Snowflake external function translators for changing the format of the data\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "What is Kedro? The MLOps Framework\n",
            "\n",
            "Kedro is a widely-adopted MLOps framework in Python, that brings engineering back to the data science world to help productionize machine learning code seamlessly. Kedro lets you build machine learning pipelines that can work on cloud, edge or on-premises platforms. It's open source and offers tools for data scientists and engineers to create, share and collaborate on machine learning workflows. Additionally, Kedro allows you to track the entire machine learning lifecycle from data preparation to model deployment. (see also: Run machine learning pipelines on ‚ùÑ Snowflake using Kedro üîß MLOPS TUTORIAL )\n",
            "\n",
            "Z komentarzem [1]: zr√≥bmy embed filmu\n",
            "\n",
            "What is Kedro? The MLOps Framework\n",
            "\n",
            "Kedro is a widely-adopted MLOps framework in Python, that brings engineering back to the data science world to help productionize machine learning code seamlessly. Kedro lets you build machine learning pipelines that can work on cloud, edge or on-premises platforms. It's open source and offers tools for data scientists and engineers to create, share and collaborate on machine learning workflows. Additionally, Kedro allows you to track the entire machine learning lifecycle from data preparation to model deployment. (see also: Run machine learning pipelines on ‚ùÑ Snowflake using Kedro üîß MLOPS TUTORIAL )\n",
            "\n",
            "Z komentarzem [1]: zr√≥bmy embed filmu\n",
            "\n",
            "Z komentarzem [1]: zr√≥bmy embed filmu\n",
            "\n",
            "Kedro is a tool that can make your Machine Learning projects more scalable and flexible, while keeping things simple. It can run on any platform, whether cloud or edge computing and is designed to be easily scalable. With Kedro, data scientists and engineers can build machine learning workflows compatible with different platforms without worrying about scalability. In addition, Kedro provides flexibility in building machine learning workflows, supporting various data sources, models, and deployment targets. It makes it easier for teams to experiment with new technologies and techniques while maintaining a consistent pipeline.\n",
            "\n",
            "What is MLflow? The platform for machine learning lifecycle management\n",
            "\n",
            "Z komentarzem [1]: zr√≥bmy embed filmu\n",
            "\n",
            "Kedro is a tool that can make your Machine Learning projects more scalable and flexible, while keeping things simple. It can run on any platform, whether cloud or edge computing and is designed to be easily scalable. With Kedro, data scientists and engineers can build machine learning workflows compatible with different platforms without worrying about scalability. In addition, Kedro provides flexibility in building machine learning workflows, supporting various data sources, models, and deployment targets. It makes it easier for teams to experiment with new technologies and techniques while maintaining a consistent pipeline.\n",
            "\n",
            "What is MLflow? The platform for machine learning lifecycle management\n",
            "\n",
            "Running Kedro on Kubeflow Pipelines\n",
            "\n",
            "GitHub: https://github.com/getindata/kedro-kubeflow Documentation: https://kedro-kubeflow.readthedocs.io/en/stable/\n",
            "\n",
            "By using the Kedro-Kubeflow plug-in, we enable users to automatically translate Kedro pipelines into Kubeflow Pipelines and run them on KFP clusters. All of the nodes can have distinct CPU, memory or GPU requirements, which enable ML Engineers to easily scale the pipeline in computationally-heavy places, while light nodes might run on lower resources. As Kubeflow Pipelines itself is cloud-agnostic, you will be able to run and scale your Kedro pipelines in any Kubernetes cluster - either in-cloud (AWS, GCP, Azure) or on-prem.\n",
            "\n",
            "Kedro Spaceflights Tutorial running on Kubeflow Pipelines Kasia - ALT \"machine learning pipelines kedro kubflow plugin getindata\"\n",
            "\n",
            "Running Kedro on Airflow (in Kubernetes)\n",
            "\n",
            "GitHub: https://github.com/getindata/kedro-airflow-k8s Documentation: https://kedro-airflow-k8s.readthedocs.io/en/0.8.0/\n",
            "\n",
            "Running Kedro on Kubeflow Pipelines\n",
            "\n",
            "GitHub: https://github.com/getindata/kedro-kubeflow Documentation: https://kedro-kubeflow.readthedocs.io/en/stable/\n",
            "\n",
            "By using the Kedro-Kubeflow plug-in, we enable users to automatically translate Kedro pipelines into Kubeflow Pipelines and run them on KFP clusters. All of the nodes can have distinct CPU, memory or GPU requirements, which enable ML Engineers to easily scale the pipeline in computationally-heavy places, while light nodes might run on lower resources. As Kubeflow Pipelines itself is cloud-agnostic, you will be able to run and scale your Kedro pipelines in any Kubernetes cluster - either in-cloud (AWS, GCP, Azure) or on-prem.\n",
            "\n",
            "Kedro Spaceflights Tutorial running on Kubeflow Pipelines Kasia - ALT \"machine learning pipelines kedro kubflow plugin getindata\"\n",
            "\n",
            "Running Kedro on Airflow (in Kubernetes)\n",
            "\n",
            "GitHub: https://github.com/getindata/kedro-airflow-k8s Documentation: https://kedro-airflow-k8s.readthedocs.io/en/0.8.0/\n",
            "\n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our GitHub repos).\n",
            "\n",
            "Z komentarzem [3]: dajmy bitly na to https://github.com/getindata\n",
            "\n",
            "The below table summarizes the pros and cons of different approaches:\n",
            "\n",
            "SageMaker/Azure ML/Vertex AI\n",
            "\n",
            "Airflow\n",
            "\n",
            "Snowflake/Snowpark\n",
            "\n",
            "Orchestration\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Native data processing\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "Docker support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Native ML capabilities\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Model deployment support (serving)\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Maintenance\n",
            "\n",
            "High\n",
            "\n",
            "Low (serverless)\n",
            "\n",
            "Low / Medium\n",
            "\n",
            "Extensibility / Customizability\n",
            "\n",
            "High\n",
            "\n",
            "Low\n",
            "\n",
            "Medium\n",
            "\n",
            "Performance\n",
            "\n",
            "Depends on setup\n",
            "\n",
            "Varies\n",
            "\n",
            "Low to very high\n",
            "\n",
            "Experiment tracking\n",
            "\n",
            "External\n",
            "\n",
            "Built-in\n",
            "\n",
            "External\n",
            "\n",
            "GPU support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Python + Any (Docker)\n",
            "\n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our GitHub repos).\n",
            "\n",
            "Z komentarzem [3]: dajmy bitly na to https://github.com/getindata\n",
            "\n",
            "The below table summarizes the pros and cons of different approaches:\n",
            "\n",
            "SageMaker/Azure ML/Vertex AI\n",
            "\n",
            "Airflow\n",
            "\n",
            "Snowflake/Snowpark\n",
            "\n",
            "Orchestration\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Native data processing\n",
            "\n",
            "0\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "Docker support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Native ML capabilities\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Model deployment support (serving)\n",
            "\n",
            "0\n",
            "\n",
            "1\n",
            "\n",
            "0.5\n",
            "\n",
            "Maintenance\n",
            "\n",
            "High\n",
            "\n",
            "Low (serverless)\n",
            "\n",
            "Low / Medium\n",
            "\n",
            "Extensibility / Customizability\n",
            "\n",
            "High\n",
            "\n",
            "Low\n",
            "\n",
            "Medium\n",
            "\n",
            "Performance\n",
            "\n",
            "Depends on setup\n",
            "\n",
            "Varies\n",
            "\n",
            "Low to very high\n",
            "\n",
            "Experiment tracking\n",
            "\n",
            "External\n",
            "\n",
            "Built-in\n",
            "\n",
            "External\n",
            "\n",
            "GPU support\n",
            "\n",
            "1\n",
            "\n",
            "1\n",
            "\n",
            "0\n",
            "\n",
            "Python + Any (Docker)\n",
            "=============\n",
            "\n",
            "Question: How to integrate kedro with MLFlow?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: How to integrate kedro with MLFlow?\n",
            "................................................................................\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #0\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.8022729158401489\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com  GCP and Azure\n",
            "deployment scenarios are very much the same except for:  API Gateway (GCP) or API Management (Azure) for exposing MLflow\n",
            "API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure  Container Instances (Azure)\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints  Technical deep-dive into kedro-snowflake <-> MLflow\n",
            "integration  Kedro-snowflake <-> MLflow integration is based on the following concepts:  Snowflake external functions\n",
            "that are used for wrapping POST requests to the  MLflow instance. In the minimal setup the following wrapping external\n",
            "functions  for MLflow REST API calls must be created:  Create run  Update run  Log param  Log metric  Search experiment\n",
            "Snowflake external function translators for changing the format of the data  sent/received from the MLflow instance.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #1\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.8022729158401489\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com  GCP and Azure\n",
            "deployment scenarios are very much the same except for:  API Gateway (GCP) or API Management (Azure) for exposing MLflow\n",
            "API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure  Container Instances (Azure)\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints  Technical deep-dive into kedro-snowflake <-> MLflow\n",
            "integration  Kedro-snowflake <-> MLflow integration is based on the following concepts:  Snowflake external functions\n",
            "that are used for wrapping POST requests to the  MLflow instance. In the minimal setup the following wrapping external\n",
            "functions  for MLflow REST API calls must be created:  Create run  Update run  Log param  Log metric  Search experiment\n",
            "Snowflake external function translators for changing the format of the data  sent/received from the MLflow instance.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #2\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7778583765029907\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "What is Kedro? The MLOps Framework  Kedro is a widely-adopted MLOps framework in Python, that brings engineering back to\n",
            "the data science world to help productionize machine learning code seamlessly. Kedro lets you build machine learning\n",
            "pipelines that can work on cloud, edge or on-premises platforms. It's open source and offers tools for data scientists\n",
            "and engineers to create, share and collaborate on machine learning workflows. Additionally, Kedro allows you to track\n",
            "the entire machine learning lifecycle from data preparation to model deployment. (see also: Run machine learning\n",
            "pipelines on ‚ùÑ Snowflake using Kedro üîß MLOPS TUTORIAL )  Z komentarzem [1]: zr√≥bmy embed filmu\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #3\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7778583765029907\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "What is Kedro? The MLOps Framework  Kedro is a widely-adopted MLOps framework in Python, that brings engineering back to\n",
            "the data science world to help productionize machine learning code seamlessly. Kedro lets you build machine learning\n",
            "pipelines that can work on cloud, edge or on-premises platforms. It's open source and offers tools for data scientists\n",
            "and engineers to create, share and collaborate on machine learning workflows. Additionally, Kedro allows you to track\n",
            "the entire machine learning lifecycle from data preparation to model deployment. (see also: Run machine learning\n",
            "pipelines on ‚ùÑ Snowflake using Kedro üîß MLOPS TUTORIAL )  Z komentarzem [1]: zr√≥bmy embed filmu\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #4\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.776577353477478\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [1]: zr√≥bmy embed filmu  Kedro is a tool that can make your Machine Learning projects more scalable and\n",
            "flexible, while keeping things simple. It can run on any platform, whether cloud or edge computing and is designed to be\n",
            "easily scalable. With Kedro, data scientists and engineers can build machine learning workflows compatible with\n",
            "different platforms without worrying about scalability. In addition, Kedro provides flexibility in building machine\n",
            "learning workflows, supporting various data sources, models, and deployment targets. It makes it easier for teams to\n",
            "experiment with new technologies and techniques while maintaining a consistent pipeline.  What is MLflow? The platform\n",
            "for machine learning lifecycle management\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #5\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.776577353477478\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [1]: zr√≥bmy embed filmu  Kedro is a tool that can make your Machine Learning projects more scalable and\n",
            "flexible, while keeping things simple. It can run on any platform, whether cloud or edge computing and is designed to be\n",
            "easily scalable. With Kedro, data scientists and engineers can build machine learning workflows compatible with\n",
            "different platforms without worrying about scalability. In addition, Kedro provides flexibility in building machine\n",
            "learning workflows, supporting various data sources, models, and deployment targets. It makes it easier for teams to\n",
            "experiment with new technologies and techniques while maintaining a consistent pipeline.  What is MLflow? The platform\n",
            "for machine learning lifecycle management\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #6\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7759016156196594\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: Running Kedro... everywhere - blogpost.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Running Kedro on Kubeflow Pipelines  GitHub: https://github.com/getindata/kedro-kubeflow Documentation: https://kedro-\n",
            "kubeflow.readthedocs.io/en/stable/  By using the Kedro-Kubeflow plug-in, we enable users to automatically translate\n",
            "Kedro pipelines into Kubeflow Pipelines and run them on KFP clusters. All of the nodes can have distinct CPU, memory or\n",
            "GPU requirements, which enable ML Engineers to easily scale the pipeline in computationally-heavy places, while light\n",
            "nodes might run on lower resources. As Kubeflow Pipelines itself is cloud-agnostic, you will be able to run and scale\n",
            "your Kedro pipelines in any Kubernetes cluster - either in-cloud (AWS, GCP, Azure) or on-prem.  Kedro Spaceflights\n",
            "Tutorial running on Kubeflow Pipelines Kasia - ALT \"machine learning pipelines kedro kubflow plugin getindata\"  Running\n",
            "Kedro on Airflow (in Kubernetes)  GitHub: https://github.com/getindata/kedro-airflow-k8s Documentation: https://kedro-\n",
            "airflow-k8s.readthedocs.io/en/0.8.0/\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #7\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7759016156196594\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: Running Kedro... everywhere - blogpost.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Running Kedro on Kubeflow Pipelines  GitHub: https://github.com/getindata/kedro-kubeflow Documentation: https://kedro-\n",
            "kubeflow.readthedocs.io/en/stable/  By using the Kedro-Kubeflow plug-in, we enable users to automatically translate\n",
            "Kedro pipelines into Kubeflow Pipelines and run them on KFP clusters. All of the nodes can have distinct CPU, memory or\n",
            "GPU requirements, which enable ML Engineers to easily scale the pipeline in computationally-heavy places, while light\n",
            "nodes might run on lower resources. As Kubeflow Pipelines itself is cloud-agnostic, you will be able to run and scale\n",
            "your Kedro pipelines in any Kubernetes cluster - either in-cloud (AWS, GCP, Azure) or on-prem.  Kedro Spaceflights\n",
            "Tutorial running on Kubeflow Pipelines Kasia - ALT \"machine learning pipelines kedro kubflow plugin getindata\"  Running\n",
            "Kedro on Airflow (in Kubernetes)  GitHub: https://github.com/getindata/kedro-airflow-k8s Documentation: https://kedro-\n",
            "airflow-k8s.readthedocs.io/en/0.8.0/\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #8\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7629930377006531\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and\n",
            "switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for\n",
            "Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our\n",
            "GitHub repos).  Z komentarzem [3]: dajmy bitly na to https://github.com/getindata  The below table summarizes the pros\n",
            "and cons of different approaches:  SageMaker/Azure ML/Vertex AI  Airflow  Snowflake/Snowpark  Orchestration  1  1  0.5\n",
            "Native data processing  0  0  1  Docker support  1  1  0  Native ML capabilities  0  1  0.5  Model deployment support\n",
            "(serving)  0  1  0.5  Maintenance  High  Low (serverless)  Low / Medium  Extensibility / Customizability  High  Low\n",
            "Medium  Performance  Depends on setup  Varies  Low to very high  Experiment tracking  External  Built-in  External  GPU\n",
            "support  1  1  0  Python + Any (Docker)\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #9\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7629930377006531\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "By building your core ML project and delivering the business value on top of the Kedro framework, later migration and\n",
            "switching between those two approaches is possible with minimal effort. So far, we‚Äôve open sourced 6 major plugins for\n",
            "Kedro: Kedro-Snowflake, Kedro-AzureML, Kedro-VertexAI, Kedro-SageMaker, Kedro-Airflow and Kedro-Kubeflow (check our\n",
            "GitHub repos).  Z komentarzem [3]: dajmy bitly na to https://github.com/getindata  The below table summarizes the pros\n",
            "and cons of different approaches:  SageMaker/Azure ML/Vertex AI  Airflow  Snowflake/Snowpark  Orchestration  1  1  0.5\n",
            "Native data processing  0  0  1  Docker support  1  1  0  Native ML capabilities  0  1  0.5  Model deployment support\n",
            "(serving)  0  1  0.5  Maintenance  High  Low (serverless)  Low / Medium  Extensibility / Customizability  High  Low\n",
            "Medium  Performance  Depends on setup  Varies  Low to very high  Experiment tracking  External  Built-in  External  GPU\n",
            "support  1  1  0  Python + Any (Docker)\n",
            "................................................................................\n",
            "Response: I cannot determine the answer to that.\n",
            "................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"How to integrate MLflow with Snowflake?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhbB4QtfY5fm",
        "outputId": "6ff48bc0-66b4-4603-97da-1d7d8832d680"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Waiting\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
            "\n",
            "Question: How to integrate MLflow with Snowflake?\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "Snowflake API integration for setting up a communication channel from the\n",
            "\n",
            "Snowflake instance to the cloud HTTPS proxy/gateway service where your\n",
            "\n",
            "MLflow instance is hosted (e.g. Amazon API Gateway, Google Cloud API\n",
            "\n",
            "Gateway or Azure API Management).\n",
            "\n",
            "Snowflake storage integration to enable your Snowflake instance to upload\n",
            "\n",
            "artifacts (e.g. serialized models) to the cloud storage (Amazon S3, Azure Blob\n",
            "\n",
            "Storage, Google Cloud Storage) used by the MLflow instance.\n",
            "\n",
            "Offline model deployment within Snowflake is handled by the MLflow-Snowflake\n",
            "\n",
            "plugin, which allows you to deploy models trained in popular frameworks (such as\n",
            "\n",
            "PyTorch, Scikit-Learn, TensorFlow, LightGBM, XGBoost, ONNX and others)\n",
            "\n",
            "natively in Snowflake as User-Defined Functions, which then can be called\n",
            "\n",
            "directly from SQL to efficiently (using vectorization) perform inference and obtain\n",
            "\n",
            "the models‚Äô predictions. The plugin is backed by the Snowflake team and is\n",
            "\n",
            "actively developed.\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "Snowflake API integration for setting up a communication channel from the\n",
            "\n",
            "Snowflake instance to the cloud HTTPS proxy/gateway service where your\n",
            "\n",
            "MLflow instance is hosted (e.g. Amazon API Gateway, Google Cloud API\n",
            "\n",
            "Gateway or Azure API Management).\n",
            "\n",
            "Snowflake storage integration to enable your Snowflake instance to upload\n",
            "\n",
            "artifacts (e.g. serialized models) to the cloud storage (Amazon S3, Azure Blob\n",
            "\n",
            "Storage, Google Cloud Storage) used by the MLflow instance.\n",
            "\n",
            "Offline model deployment within Snowflake is handled by the MLflow-Snowflake\n",
            "\n",
            "plugin, which allows you to deploy models trained in popular frameworks (such as\n",
            "\n",
            "PyTorch, Scikit-Learn, TensorFlow, LightGBM, XGBoost, ONNX and others)\n",
            "\n",
            "natively in Snowflake as User-Defined Functions, which then can be called\n",
            "\n",
            "directly from SQL to efficiently (using vectorization) perform inference and obtain\n",
            "\n",
            "the models‚Äô predictions. The plugin is backed by the Snowflake team and is\n",
            "\n",
            "actively developed.\n",
            "\n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com\n",
            "\n",
            "GCP and Azure deployment scenarios are very much the same except for:\n",
            "\n",
            "API Gateway (GCP) or API Management (Azure) for exposing MLflow API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure\n",
            "\n",
            "Container Instances (Azure)\n",
            "\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints\n",
            "\n",
            "Technical deep-dive into kedro-snowflake <-> MLflow integration\n",
            "\n",
            "Kedro-snowflake <-> MLflow integration is based on the following concepts:\n",
            "\n",
            "Snowflake external functions that are used for wrapping POST requests to the\n",
            "\n",
            "MLflow instance. In the minimal setup the following wrapping external functions\n",
            "\n",
            "for MLflow REST API calls must be created:\n",
            "\n",
            "Create run\n",
            "\n",
            "Update run\n",
            "\n",
            "Log param\n",
            "\n",
            "Log metric\n",
            "\n",
            "Search experiment\n",
            "\n",
            "Snowflake external function translators for changing the format of the data\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com\n",
            "\n",
            "GCP and Azure deployment scenarios are very much the same except for:\n",
            "\n",
            "API Gateway (GCP) or API Management (Azure) for exposing MLflow API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure\n",
            "\n",
            "Container Instances (Azure)\n",
            "\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints\n",
            "\n",
            "Technical deep-dive into kedro-snowflake <-> MLflow integration\n",
            "\n",
            "Kedro-snowflake <-> MLflow integration is based on the following concepts:\n",
            "\n",
            "Snowflake external functions that are used for wrapping POST requests to the\n",
            "\n",
            "MLflow instance. In the minimal setup the following wrapping external functions\n",
            "\n",
            "for MLflow REST API calls must be created:\n",
            "\n",
            "Create run\n",
            "\n",
            "Update run\n",
            "\n",
            "Log param\n",
            "\n",
            "Log metric\n",
            "\n",
            "Search experiment\n",
            "\n",
            "Snowflake external function translators for changing the format of the data\n",
            "\n",
            "sent/received from the MLflow instance.\n",
            "\n",
            "MLOps, MLOps platform, snowflake, snowflake data cloud, kedro\n",
            "\n",
            "Tags\n",
            "\n",
            "How to implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow.\n",
            "\n",
            "Meta Description\n",
            "\n",
            "How to implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow. Dive into the cloud- agnostic MLOps Snowflake Platform\n",
            "\n",
            "Intro KB\n",
            "\n",
            "Marek Wiewi√≥rka Marcin Zab≈ÇockiMicha≈Ç Bry≈õ\n",
            "\n",
            "Autor\n",
            "\n",
            "LINK do grafik\n",
            "\n",
            "From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-agnostic MLOps Platform for Snowflake Data Cloud\n",
            "\n",
            "From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-agnostic MLOps Platform for Snowflake Data Cloud\n",
            "\n",
            "1 2 2 2 3 3\n",
            "\n",
            "MLOps Platform pillars - recap\n",
            "\n",
            "Kedro - the MLOps Framework MLflow - the platform for machine learning lifecycle management Terraform\n",
            "\n",
            "ML‚ùÑps Platform high level architecture Technical deep-dive into kedro-snowflake <-> MLflow integration Pros, cons and alternative approaches Summary and future improvements\n",
            "\n",
            "4 5 7\n",
            "\n",
            "From 0 to MLOps with Snowflake ‚ùÑ\n",
            "\n",
            "MLOps, MLOps platform, snowflake, snowflake data cloud, kedro\n",
            "\n",
            "Tags\n",
            "\n",
            "How to implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow.\n",
            "\n",
            "Meta Description\n",
            "\n",
            "How to implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow. Dive into the cloud- agnostic MLOps Snowflake Platform\n",
            "\n",
            "Intro KB\n",
            "\n",
            "Marek Wiewi√≥rka Marcin Zab≈ÇockiMicha≈Ç Bry≈õ\n",
            "\n",
            "Autor\n",
            "\n",
            "LINK do grafik\n",
            "\n",
            "From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-agnostic MLOps Platform for Snowflake Data Cloud\n",
            "\n",
            "From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-agnostic MLOps Platform for Snowflake Data Cloud\n",
            "\n",
            "1 2 2 2 3 3\n",
            "\n",
            "MLOps Platform pillars - recap\n",
            "\n",
            "Kedro - the MLOps Framework MLflow - the platform for machine learning lifecycle management Terraform\n",
            "\n",
            "ML‚ùÑps Platform high level architecture Technical deep-dive into kedro-snowflake <-> MLflow integration Pros, cons and alternative approaches Summary and future improvements\n",
            "\n",
            "4 5 7\n",
            "\n",
            "From 0 to MLOps with Snowflake ‚ùÑ\n",
            "\n",
            "actively developed.\n",
            "\n",
            "Online model deployment with SageMaker Endpoints with the MLflow official\n",
            "\n",
            "plugin.\n",
            "\n",
            "YOU‚ìá MLOps Snowflake Platform: Pros, cons and alternative\n",
            "\n",
            "approaches\n",
            "\n",
            "Our MLOps platform assumes a fully native integration with the Snowflake ecosystem and leverages it for data access, pipeline orchestration, model training as well as model deployment and inference. Such an architecture has a number of advantages, just to name a few:\n",
            "\n",
            "simpler security setup (also thanks to limited data egress) ‚óè ‚óè substantially less data transfers\n",
            "\n",
            "fewer dependencies on external services\n",
            "\n",
            "\n",
            "\n",
            "lower SageMaker/AzureML/Vertex AI\n",
            "\n",
            "pipeline\n",
            "\n",
            "nodes\n",
            "\n",
            "startup\n",
            "\n",
            "overheads\n",
            "\n",
            "when\n",
            "\n",
            "compared\n",
            "\n",
            "to\n",
            "\n",
            "a unified data and machine learning platform\n",
            "\n",
            "This is of course not a one-size-fits-all architecture and there are shortcomings that have not yet been addressed in the Snowflake Data Cloud, i.e.:\n",
            "\n",
            "actively developed.\n",
            "\n",
            "Online model deployment with SageMaker Endpoints with the MLflow official\n",
            "\n",
            "plugin.\n",
            "\n",
            "YOU‚ìá MLOps Snowflake Platform: Pros, cons and alternative\n",
            "\n",
            "approaches\n",
            "\n",
            "Our MLOps platform assumes a fully native integration with the Snowflake ecosystem and leverages it for data access, pipeline orchestration, model training as well as model deployment and inference. Such an architecture has a number of advantages, just to name a few:\n",
            "\n",
            "simpler security setup (also thanks to limited data egress) ‚óè ‚óè substantially less data transfers\n",
            "\n",
            "fewer dependencies on external services\n",
            "\n",
            "\n",
            "\n",
            "lower SageMaker/AzureML/Vertex AI\n",
            "\n",
            "pipeline\n",
            "\n",
            "nodes\n",
            "\n",
            "startup\n",
            "\n",
            "overheads\n",
            "\n",
            "when\n",
            "\n",
            "compared\n",
            "\n",
            "to\n",
            "\n",
            "a unified data and machine learning platform\n",
            "\n",
            "This is of course not a one-size-fits-all architecture and there are shortcomings that have not yet been addressed in the Snowflake Data Cloud, i.e.:\n",
            "\n",
            "4 5 7\n",
            "\n",
            "From 0 to MLOps with Snowflake ‚ùÑ\n",
            "\n",
            "In the first part of the blogpost, we presented our kedro-snowflake plugin that enables you to run your Kedro pipelines on the Snowflake Data Cloud in 3 simple steps. This time we are going to demonstrate how you can implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow. Inspired by the blog posts: 1 and 2 - in this article we present a novel approach that tries to address the problem of missing External Internet Access in Snowpark.\n",
            "\n",
            "This feature would be an absolute game changer for implementing any integration with third party tools not natively available in the Snowflake ecosystem and according to the best of knowledge is on the Snowflake roadmap. So let‚Äôs dive into the MLOps Platform for Snowflake Data Cloud.\n",
            "\n",
            "MLOps Platform pillars - recap\n",
            "\n",
            "What is Kedro? The MLOps Framework\n",
            "\n",
            "4 5 7\n",
            "\n",
            "From 0 to MLOps with Snowflake ‚ùÑ\n",
            "\n",
            "In the first part of the blogpost, we presented our kedro-snowflake plugin that enables you to run your Kedro pipelines on the Snowflake Data Cloud in 3 simple steps. This time we are going to demonstrate how you can implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow. Inspired by the blog posts: 1 and 2 - in this article we present a novel approach that tries to address the problem of missing External Internet Access in Snowpark.\n",
            "\n",
            "This feature would be an absolute game changer for implementing any integration with third party tools not natively available in the Snowflake ecosystem and according to the best of knowledge is on the Snowflake roadmap. So let‚Äôs dive into the MLOps Platform for Snowflake Data Cloud.\n",
            "\n",
            "MLOps Platform pillars - recap\n",
            "\n",
            "What is Kedro? The MLOps Framework\n",
            "=============\n",
            "\n",
            "Question: How to integrate MLflow with Snowflake?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: How to integrate MLflow with Snowflake?\n",
            "................................................................................\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #0\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.8213270306587219\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "sent/received from the MLflow instance.  Snowflake API integration for setting up a communication channel from the\n",
            "Snowflake instance to the cloud HTTPS proxy/gateway service where your  MLflow instance is hosted (e.g. Amazon API\n",
            "Gateway, Google Cloud API  Gateway or Azure API Management).  Snowflake storage integration to enable your Snowflake\n",
            "instance to upload  artifacts (e.g. serialized models) to the cloud storage (Amazon S3, Azure Blob  Storage, Google\n",
            "Cloud Storage) used by the MLflow instance.  Offline model deployment within Snowflake is handled by the MLflow-\n",
            "Snowflake  plugin, which allows you to deploy models trained in popular frameworks (such as  PyTorch, Scikit-Learn,\n",
            "TensorFlow, LightGBM, XGBoost, ONNX and others)  natively in Snowflake as User-Defined Functions, which then can be\n",
            "called  directly from SQL to efficiently (using vectorization) perform inference and obtain  the models‚Äô predictions.\n",
            "The plugin is backed by the Snowflake team and is  actively developed.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #1\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.8213270306587219\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "sent/received from the MLflow instance.  Snowflake API integration for setting up a communication channel from the\n",
            "Snowflake instance to the cloud HTTPS proxy/gateway service where your  MLflow instance is hosted (e.g. Amazon API\n",
            "Gateway, Google Cloud API  Gateway or Azure API Management).  Snowflake storage integration to enable your Snowflake\n",
            "instance to upload  artifacts (e.g. serialized models) to the cloud storage (Amazon S3, Azure Blob  Storage, Google\n",
            "Cloud Storage) used by the MLflow instance.  Offline model deployment within Snowflake is handled by the MLflow-\n",
            "Snowflake  plugin, which allows you to deploy models trained in popular frameworks (such as  PyTorch, Scikit-Learn,\n",
            "TensorFlow, LightGBM, XGBoost, ONNX and others)  natively in Snowflake as User-Defined Functions, which then can be\n",
            "called  directly from SQL to efficiently (using vectorization) perform inference and obtain  the models‚Äô predictions.\n",
            "The plugin is backed by the Snowflake team and is  actively developed.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #2\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7868118286132812\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com  GCP and Azure\n",
            "deployment scenarios are very much the same except for:  API Gateway (GCP) or API Management (Azure) for exposing MLflow\n",
            "API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure  Container Instances (Azure)\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints  Technical deep-dive into kedro-snowflake <-> MLflow\n",
            "integration  Kedro-snowflake <-> MLflow integration is based on the following concepts:  Snowflake external functions\n",
            "that are used for wrapping POST requests to the  MLflow instance. In the minimal setup the following wrapping external\n",
            "functions  for MLflow REST API calls must be created:  Create run  Update run  Log param  Log metric  Search experiment\n",
            "Snowflake external function translators for changing the format of the data  sent/received from the MLflow instance.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #3\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7868118286132812\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "Z komentarzem [2]: Dodalem info o 0.2.0 release with MLflow support @sylwia.kolpuc@getindata.com  GCP and Azure\n",
            "deployment scenarios are very much the same except for:  API Gateway (GCP) or API Management (Azure) for exposing MLflow\n",
            "API ‚óè MLflow hosting - e.g. App Engine, Cloud Run (GCP) or Azure Container Apps, Azure  Container Instances (Azure)\n",
            "Online model serving - Vertex Endpoints (GCP) or Azure ML Endpoints  Technical deep-dive into kedro-snowflake <-> MLflow\n",
            "integration  Kedro-snowflake <-> MLflow integration is based on the following concepts:  Snowflake external functions\n",
            "that are used for wrapping POST requests to the  MLflow instance. In the minimal setup the following wrapping external\n",
            "functions  for MLflow REST API calls must be created:  Create run  Update run  Log param  Log metric  Search experiment\n",
            "Snowflake external function translators for changing the format of the data  sent/received from the MLflow instance.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #4\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7834271192550659\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "MLOps, MLOps platform, snowflake, snowflake data cloud, kedro  Tags  How to implement the end-to-end MLOps platform on\n",
            "top of Snowflake powered by Kedro and MLflow.  Meta Description  How to implement the end-to-end MLOps platform on top\n",
            "of Snowflake powered by Kedro and MLflow. Dive into the cloud- agnostic MLOps Snowflake Platform  Intro KB  Marek\n",
            "Wiewi√≥rka Marcin Zab≈ÇockiMicha≈Ç Bry≈õ  Autor  LINK do grafik  From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-\n",
            "agnostic MLOps Platform for Snowflake Data Cloud  From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-agnostic MLOps\n",
            "Platform for Snowflake Data Cloud  1 2 2 2 3 3  MLOps Platform pillars - recap  Kedro - the MLOps Framework MLflow - the\n",
            "platform for machine learning lifecycle management Terraform  ML‚ùÑps Platform high level architecture Technical deep-dive\n",
            "into kedro-snowflake <-> MLflow integration Pros, cons and alternative approaches Summary and future improvements  4 5 7\n",
            "From 0 to MLOps with Snowflake ‚ùÑ\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #5\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7834271192550659\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "MLOps, MLOps platform, snowflake, snowflake data cloud, kedro  Tags  How to implement the end-to-end MLOps platform on\n",
            "top of Snowflake powered by Kedro and MLflow.  Meta Description  How to implement the end-to-end MLOps platform on top\n",
            "of Snowflake powered by Kedro and MLflow. Dive into the cloud- agnostic MLOps Snowflake Platform  Intro KB  Marek\n",
            "Wiewi√≥rka Marcin Zab≈ÇockiMicha≈Ç Bry≈õ  Autor  LINK do grafik  From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-\n",
            "agnostic MLOps Platform for Snowflake Data Cloud  From 0 to MLOps with ‚ùÑ Part 2: Architecting the cloud-agnostic MLOps\n",
            "Platform for Snowflake Data Cloud  1 2 2 2 3 3  MLOps Platform pillars - recap  Kedro - the MLOps Framework MLflow - the\n",
            "platform for machine learning lifecycle management Terraform  ML‚ùÑps Platform high level architecture Technical deep-dive\n",
            "into kedro-snowflake <-> MLflow integration Pros, cons and alternative approaches Summary and future improvements  4 5 7\n",
            "From 0 to MLOps with Snowflake ‚ùÑ\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #6\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7790540456771851\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "actively developed.  Online model deployment with SageMaker Endpoints with the MLflow official  plugin.  YOU‚ìá MLOps\n",
            "Snowflake Platform: Pros, cons and alternative  approaches  Our MLOps platform assumes a fully native integration with\n",
            "the Snowflake ecosystem and leverages it for data access, pipeline orchestration, model training as well as model\n",
            "deployment and inference. Such an architecture has a number of advantages, just to name a few:  simpler security setup\n",
            "(also thanks to limited data egress) ‚óè ‚óè substantially less data transfers  fewer dependencies on external services\n",
            "lower SageMaker/AzureML/Vertex AI  pipeline  nodes  startup  overheads  when  compared  to  a unified data and machine\n",
            "learning platform  This is of course not a one-size-fits-all architecture and there are shortcomings that have not yet\n",
            "been addressed in the Snowflake Data Cloud, i.e.:\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #7\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7790540456771851\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "actively developed.  Online model deployment with SageMaker Endpoints with the MLflow official  plugin.  YOU‚ìá MLOps\n",
            "Snowflake Platform: Pros, cons and alternative  approaches  Our MLOps platform assumes a fully native integration with\n",
            "the Snowflake ecosystem and leverages it for data access, pipeline orchestration, model training as well as model\n",
            "deployment and inference. Such an architecture has a number of advantages, just to name a few:  simpler security setup\n",
            "(also thanks to limited data egress) ‚óè ‚óè substantially less data transfers  fewer dependencies on external services\n",
            "lower SageMaker/AzureML/Vertex AI  pipeline  nodes  startup  overheads  when  compared  to  a unified data and machine\n",
            "learning platform  This is of course not a one-size-fits-all architecture and there are shortcomings that have not yet\n",
            "been addressed in the Snowflake Data Cloud, i.e.:\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #8\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7748053073883057\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "4 5 7  From 0 to MLOps with Snowflake ‚ùÑ  In the first part of the blogpost, we presented our kedro-snowflake plugin that\n",
            "enables you to run your Kedro pipelines on the Snowflake Data Cloud in 3 simple steps. This time we are going to\n",
            "demonstrate how you can implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow.\n",
            "Inspired by the blog posts: 1 and 2 - in this article we present a novel approach that tries to address the problem of\n",
            "missing External Internet Access in Snowpark.  This feature would be an absolute game changer for implementing any\n",
            "integration with third party tools not natively available in the Snowflake ecosystem and according to the best of\n",
            "knowledge is on the Snowflake roadmap. So let‚Äôs dive into the MLOps Platform for Snowflake Data Cloud.  MLOps Platform\n",
            "pillars - recap  What is Kedro? The MLOps Framework\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE #9\n",
            "--------------------------------------------------------------------------------\n",
            "Matching Score: 0.7748053073883057\n",
            "Document Source: datamass-lab-2023/\n",
            "Document Name: MLOps@Snowflake blog post.pdf\n",
            "................................................................................\n",
            "Content: \n",
            "4 5 7  From 0 to MLOps with Snowflake ‚ùÑ  In the first part of the blogpost, we presented our kedro-snowflake plugin that\n",
            "enables you to run your Kedro pipelines on the Snowflake Data Cloud in 3 simple steps. This time we are going to\n",
            "demonstrate how you can implement the end-to-end MLOps platform on top of Snowflake powered by Kedro and MLflow.\n",
            "Inspired by the blog posts: 1 and 2 - in this article we present a novel approach that tries to address the problem of\n",
            "missing External Internet Access in Snowpark.  This feature would be an absolute game changer for implementing any\n",
            "integration with third party tools not natively available in the Snowflake ecosystem and according to the best of\n",
            "knowledge is on the Snowflake roadmap. So let‚Äôs dive into the MLOps Platform for Snowflake Data Cloud.  MLOps Platform\n",
            "pillars - recap  What is Kedro? The MLOps Framework\n",
            "................................................................................\n",
            "Response: To integrate MLflow with Snowflake, you can use the MLflow-Snowflake plugin. This plugin allows you to deploy models\n",
            "trained in popular frameworks (such as PyTorch, Scikit-Learn, TensorFlow, LightGBM, XGBoost, ONNX and others) natively\n",
            "in Snowflake as User-Defined Functions, which then can be called directly from SQL to efficiently (using vectorization)\n",
            "perform inference and obtain the models' predictions. The plugin is backed by the Snowflake team and is actively\n",
            "developed.\n",
            "................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question outside the knowledge domain"
      ],
      "metadata": {
        "id": "XTL7eDnabV5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What is NFC?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alSLUZN5Y97M",
        "outputId": "840566da-d5e9-4ae6-e2b2-e03edec4df3b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Waiting\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSYSTEM: You are an intelligent assistant helping the users with their questions on research papers.\n",
            "\n",
            "Question: What is NFC?\n",
            "\n",
            "Strictly Use ONLY the following pieces of context to answer the question at the end. Think step-by-step and then answer.\n",
            "\n",
            "Do not try to make up an answer:\n",
            " - If the answer to the question cannot be determined from the context alone, say \"I cannot determine the answer to that.\"\n",
            " - If the context is empty, just say \"I do not know the answer to that.\"\n",
            "\n",
            "=============\n",
            "\n",
            "=============\n",
            "\n",
            "Question: What is NFC?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Query: What is NFC?\n",
            "................................................................................\n",
            "................................................................................\n",
            "Response: I cannot determine the answer to that.\n",
            "................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference: LLM without external knowledge base"
      ],
      "metadata": {
        "id": "adSumTRxbgNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also send questions directly to the PaLM (`text-bison@001`) available in the LangChain.\n",
        "\n"
      ],
      "metadata": {
        "id": "78SXN2Naci6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text model instance integrated with langChain\n",
        "# llm = VertexAI(\n",
        "#     model_name=\"text-bison@001\",\n",
        "#     max_output_tokens=1024,\n",
        "#     temperature=0.2,\n",
        "#     top_p=0.8,\n",
        "#     top_k=40,\n",
        "#     verbose=True,\n",
        "# )\n",
        "\n",
        "print(llm(\"What are some of the pros and cons of Kedro?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-3yNkXWbY0S",
        "outputId": "ea83cdc6-f0e3-4dce-b904-ed4389ffb0d5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Pros:**\n",
            "\n",
            "* Kedro is a Python-based framework that makes it easy to build and maintain data pipelines.\n",
            "* It provides a number of features that make it easy to create modular, reusable, and scalable pipelines, such as:\n",
            "    * A DAG (directed acyclic graph) representation of the pipeline, which makes it easy to visualize and understand the flow of data.\n",
            "    * A code generator that can automatically generate Python code for the pipeline.\n",
            "    * A library of reusable components that can be used to build pipelines.\n",
            "* Kedro is open source and has a large and active community that provides support and resources.\n",
            "\n",
            "**Cons:**\n",
            "\n",
            "* Kedro is a relatively new framework, so it may not be as mature as some of the other options available.\n",
            "* It can be a bit complex to get started with, especially if you are not familiar with Python or data pipelines.\n",
            "* Kedro does not support all data sources and formats out of the box, so you may need to write custom code to support your specific needs.\n",
            "\n",
            "Overall, Kedro is a powerful and versatile framework for building data pipelines. It is well-suited for teams that need to build and maintain complex and scalable pipelines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(\"How to integrate MLflow with Snowflake?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn__t3MfcH1u",
        "outputId": "5a237ea7-54b1-4a62-9fab-efb84a23dc85"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It provides a central repository for storing and tracking experiments, as well as tools for deploying models to production. Snowflake is a cloud-based data warehouse that provides a fast and scalable platform for storing and querying data.\n",
            "\n",
            "To integrate MLflow with Snowflake, you can use the MLflow Snowflake Connector. The connector provides a simple way to connect MLflow to Snowflake and to store and query experiment data in Snowflake.\n",
            "\n",
            "To use the connector, you first need to install it. You can do this by running the following command:\n",
            "\n",
            "```\n",
            "pip install mlflow-snowflake\n",
            "```\n",
            "\n",
            "Once the connector is installed, you can connect MLflow to Snowflake by following these steps:\n",
            "\n",
            "1. Open the MLflow UI.\n",
            "2. Click on the \"Settings\" tab.\n",
            "3. In the \"Connections\" section, click on the \"Add Connection\" button.\n",
            "4. In the \"Connection Type\" drop-down menu, select \"Snowflake\".\n",
            "5. Enter the following information:\n",
            "\n",
            "* **Snowflake URL:** The URL of your Snowflake instance.\n",
            "* **Username:** Your Snowflake username.\n",
            "* **Password:** Your Snowflake password.\n",
            "* **Database:** The name of the database that you want to use.\n",
            "* **Schema:** The name of the schema that you want to use.\n",
            "\n",
            "6. Click on the \"Save\" button.\n",
            "\n",
            "Once you have connected MLflow to Snowflake, you can start storing and querying experiment data in Snowflake. To store experiment data in Snowflake, you can use the `mlflow.log_artifact()` function. This function takes the following arguments:\n",
            "\n",
            "* **artifact_path:** The path to the artifact that you want to store.\n",
            "* **artifact_type:** The type of the artifact.\n",
            "* **artifact_description:** A description of the artifact.\n",
            "\n",
            "For example, the following code will store an artifact named \"my_model.pkl\" in the \"models\" schema:\n",
            "\n",
            "```\n",
            "mlflow.log_artifact(\"models/my_model.pkl\", \"model\", \"This is my model.\")\n",
            "```\n",
            "\n",
            "To query experiment data in Snowflake, you can use the `mlflow.get_artifacts()` function. This function takes the following arguments:\n",
            "\n",
            "* **experiment_id:** The ID of the experiment that you want to query.\n",
            "* **artifact_path:** The path to the artifact that you want to query.\n",
            "\n",
            "For example, the following code will query the artifact named \"my_model.pkl\" from the experiment with ID \"123456\":\n",
            "\n",
            "```\n",
            "artifacts = mlflow.get_artifacts(\"123456\", \"models/my_model.pkl\")\n",
            "```\n",
            "\n",
            "The `mlflow.get_artifacts()` function will return a list of artifacts. Each artifact will have the following attributes:\n",
            "\n",
            "* **name:** The name of the artifact.\n",
            "* **type:** The type of the artifact.\n",
            "* **description:** A description of the artifact.\n",
            "* **uri:** The URI of the artifact.\n",
            "\n",
            "You can use the `name`, `type`, and `description` attributes to filter the results of the `mlflow.get_artifacts()` function. For example, the following code will query all artifacts of type \"model\" from the experiment with ID \"123456\":\n",
            "\n",
            "```\n",
            "artifacts = mlflow.get_artifacts(\"123456\", type=\"model\")\n",
            "```\n",
            "\n",
            "You can also use the `uri` attribute to download an artifact. For example, the following code will download the artifact named \"my_model.pkl\" from the experiment with ID \"123456\":\n",
            "\n",
            "```\n",
            "with open(\"my_model.pkl\", \"wb\") as f:\n",
            "    f.write(mlflow.get_artifact(\"123456\", \"models/my_model.pkl\").uri)\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SRPHEOUcTop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}